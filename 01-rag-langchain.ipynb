{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "# Retrieval-Augmented generation (RAG)\n",
    "\n",
    "RAG is a technique for augmenting LLM knowledge with additional, often private or real-time, data.\n",
    "\n",
    "LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model’s cutoff date, you need to augment the knowledge of the model with the specific information it needs.\n",
    "\n",
    "<img src=\"../figures/RAG-process.png\" >\n",
    "\n",
    "Introducing `ChakyBot`, an innovative chatbot designed to assist Chaky (the instructor) and TA (Gun) in explaining the lesson of the NLP course to students. Leveraging LangChain technology, ChakyBot excels in retrieving information from documents, ensuring a seamless and efficient learning experience for students engaging with the NLP curriculum.\n",
    "\n",
    "1. Prompt\n",
    "2. Retrieval\n",
    "3. Memory\n",
    "4. Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain library\n",
    "!pip install langchain==0.0.350\n",
    "!pip install langchain-community==0.0.4\n",
    "# LLM\n",
    "!pip install accelerate==0.25.0\n",
    "!pip install transformers==4.36.2\n",
    "!pip install bitsandbytes==0.45.3\n",
    "# text Embedding\n",
    "!pip install sentence-transformers==2.2.2\n",
    "!pip install InstructorEmbedding==1.0.1\n",
    "# vectorstore\n",
    "!pip install pymupdf==1.23.8\n",
    "!pip install faiss-cpu==1.7.4\n",
    "# huggingface_hub\n",
    "!pip install huggingface-hub==0.20.0\n",
    "# protobuf\n",
    "! pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# Set GPU device\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prompt\n",
    "\n",
    "A set of instructions or input provided by a user to guide the model's response, helping it understand the context and generate relevant and coherent language-based output, such as answering questions, completing sentences, or engaging in a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template='Your name is Francis. You are a friendly chatbot designed to answer questions about Phone Myint Naing because you have all information about him.\\nProvide gentle and informative answers based on the context:\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Your name is Francis. You are a friendly chatbot designed to answer questions about Phone Myint Naing because you have all information about him.\n",
    "Provide gentle and informative answers based on the context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "PROMPT = PromptTemplate.from_template(\n",
    "    template = prompt_template\n",
    ")\n",
    "\n",
    "PROMPT\n",
    "#using str.format \n",
    "#The placeholder is defined using curly brackets: {} {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Francis. You are a friendly chatbot designed to answer questions about Phone Myint Naing because you have all information about him.\\nProvide gentle and informative answers based on the context:\\n\\nContext: Burmese language is quite complex comparing to other latin languages so domain knowledge will be important when training spell correction model.\\n\\nQuestion: What are the challenges for spell correction model in Burmese? \\n\\nAnswer:'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT.format(\n",
    "    context = \"Burmese language is quite complex comparing to other latin languages so domain knowledge will be important when training spell correction model.\",\n",
    "    question = \"What are the challenges for spell correction model in Burmese? \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : [How to improve prompting (Zero-shot, Few-shot, Chain-of-Thought, etc.](https://github.com/chaklam-silpasuwanchai/Natural-Language-Processing/blob/main/Code/05%20-%20RAG/advance/cot-tot-prompting.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieval\n",
    "\n",
    "1. `Document loaders` : Load documents from many different sources (HTML, PDF, code). \n",
    "2. `Document transformers` : One of the essential steps in document retrieval is breaking down a large document into smaller, relevant chunks to enhance the retrieval process.\n",
    "3. `Text embedding models` : Embeddings capture the semantic meaning of the text, allowing you to quickly and efficiently find other pieces of text that are similar.\n",
    "4. `Vector stores`: there has emerged a need for databases to support efficient storage and searching of these embeddings.\n",
    "5. `Retrievers` : Once the data is in the database, you still need to retrieve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Document Loaders \n",
    "Use document loaders to load data from a source as Document's. A Document is a piece of text and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video.\n",
    "\n",
    "[PDF Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf)\n",
    "\n",
    "[Download Document](https://web.stanford.edu/~jurafsky/slp3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "nlp_docs = 'resume.pdf'\n",
    "\n",
    "loader = PyMuPDFLoader(nlp_docs)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Machine Learning/ Deep Learning\\nNatural Language Processing\\nComputer Vision\\nMathematics\\nPython, Java\\nLinux Servers\\nCloud Computing\\nWeb Development\\nWeb scraping \\n+650-832-597-730\\nfrancisphone1998@gmail.com\\nhttps://github.com/FrancisPhone\\nAIT, Khlong Luang District, Pathum\\nThani 12120, Thailand\\nContact\\nSkills \\nAccomplished AI/NLP professional with a\\nstrong foundation in mathematics and a\\nbackground in tutoring. Led groundbreaking\\nprojects in Burmese machine translation and\\nneural spell checking, driving advancements\\nin language technology. Known for precision\\nin algorithm design, a relentless pursuit of\\nexcellence, and the ability to tackle complex\\nAI challenges. Passionate about innovation\\nand thriving in fast-paced, collaborative\\nenvironments. Eager to contribute expertise\\nto cutting-edge AI teams shaping the future\\nof technology.\\nProfile\\nELYSIAN EDU\\nLecturer\\nLed transformative initiatives in machine\\ntranslation and neural-based spell checking,\\nparticularly for the Burmese language.\\nDeveloped integration website for NLP projects \\nBuilt efficient web scraping pipelines \\nExplored and Analyzed data for NLP projects\\nResearched new technologies for improving NLP\\nprojects\\nPython for beginners and data science classes\\nBuilt curriculums and taught as a lecturer \\nCreated and evaluated the personal projects for each\\ncourse \\n( June 2023- July 2024)\\nEducation\\nExperience\\nPhone Myint\\nNaing\\nA I / M L  E n g i n e e r\\nWEST YANGON TECHNOLOGY UNIVERSITY\\nUMG MYANMAR CO.LTD\\nBachelor in Electronic Engineering\\nAI Engineer/NLP Researcher\\n(2014-2020)\\n(May  2022- Jan 2024 )\\nMYANMAR INSTITUTE OF BUSINESS\\nDiploma in Artificial Intelligence (ABE Endorsed)\\n(2022)\\nASIAN INSTUTE OF TECHNOLOGY (THAILAND)\\nMaster in Data Science and Artificial Intelligence\\n(2024-Now)\\n', metadata={'source': 'resume.pdf', 'file_path': 'resume.pdf', 'page': 0, 'total_pages': 3, 'format': 'PDF 1.4', 'title': 'Web developer or engineer who works with both the front and back ends of a website or application. Provide an end-to-end service, and can be involved in projects that involve databases and building user-facing websites.', 'author': 'francis phone', 'subject': '', 'keywords': 'DAF5o1BzgP8,BAFe3tEswW0,0', 'creator': 'Canva', 'producer': 'Canva', 'creationDate': \"D:20250316125555+00'00'\", 'modDate': \"D:20250316125553+00'00'\", 'trapped': ''})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Document Transformers\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "doc = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Machine Learning/ Deep Learning\\nNatural Language Processing\\nComputer Vision\\nMathematics\\nPython, Java\\nLinux Servers\\nCloud Computing\\nWeb Development\\nWeb scraping \\n+650-832-597-730\\nfrancisphone1998@gmail.com\\nhttps://github.com/FrancisPhone\\nAIT, Khlong Luang District, Pathum\\nThani 12120, Thailand\\nContact\\nSkills \\nAccomplished AI/NLP professional with a\\nstrong foundation in mathematics and a\\nbackground in tutoring. Led groundbreaking\\nprojects in Burmese machine translation and\\nneural spell checking, driving advancements\\nin language technology. Known for precision\\nin algorithm design, a relentless pursuit of\\nexcellence, and the ability to tackle complex\\nAI challenges. Passionate about innovation', metadata={'source': 'resume.pdf', 'file_path': 'resume.pdf', 'page': 0, 'total_pages': 3, 'format': 'PDF 1.4', 'title': 'Web developer or engineer who works with both the front and back ends of a website or application. Provide an end-to-end service, and can be involved in projects that involve databases and building user-facing websites.', 'author': 'francis phone', 'subject': '', 'keywords': 'DAF5o1BzgP8,BAFe3tEswW0,0', 'creator': 'Canva', 'producer': 'Canva', 'creationDate': \"D:20250316125555+00'00'\", 'modDate': \"D:20250316125553+00'00'\", 'trapped': ''})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Text Embedding Models\n",
    "Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.\n",
    "\n",
    "*Note* Instructor Model : [Huggingface](gingface.co/hkunlp/instructor-base) | [Paper](https://arxiv.org/abs/2212.09741)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\InstructorEmbedding\\instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import InstructorEmbedding\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "model_name = 'hkunlp/instructor-base'\n",
    "\n",
    "embedding_model = HuggingFaceInstructEmbeddings(\n",
    "    model_name = model_name,\n",
    "    model_kwargs = {\"device\" : device}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Vector Stores\n",
    "\n",
    "One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locate vectorstore\n",
    "vector_path = 'vector-store'\n",
    "if not os.path.exists(vector_path):\n",
    "    os.makedirs(vector_path)\n",
    "    print('create path done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save vector locally\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents = doc,\n",
    "    embedding = embedding_model\n",
    ")\n",
    "\n",
    "db_file_name = 'nlp_ait'\n",
    "\n",
    "vectordb.save_local(\n",
    "    folder_path = os.path.join(vector_path, db_file_name),\n",
    "    index_name = 'nlp' #default index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 retrievers\n",
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling vector from local\n",
    "vector_path = 'vector-store'\n",
    "db_file_name = 'nlp_ait'\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectordb = FAISS.load_local(\n",
    "    folder_path = os.path.join(vector_path, db_file_name),\n",
    "    embeddings = embedding_model,\n",
    "    index_name = 'nlp' #default index\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ready to use\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Hi, I'm Phone Myint Naing, a 27-year-old Master's student in Data Science and AI at\\nthe Asian Institute of Technology (Thailand). My background is in Electronics and\\nCommunication Engineering (Myanmar), and I’ve been working as a machine\\nlearning engineer for three years—two years in NLP (full-time in Myanmar) and one\\nyear in Computer Vision (part-time, remote from Thailand).\\nCurrently, I work as an AI researcher, focusing on building machine learning and\\ndeep learning models. My main project involves extracting information from Thai\\ndocuments like National ID cards, House Registrations, Bank Passbooks, and\\nInvoices. I train models for Optical Character Recognition (OCR) and Text-to-Text\", metadata={'source': 'resume.pdf', 'file_path': 'resume.pdf', 'page': 2, 'total_pages': 3, 'format': 'PDF 1.4', 'title': 'Web developer or engineer who works with both the front and back ends of a website or application. Provide an end-to-end service, and can be involved in projects that involve databases and building user-facing websites.', 'author': 'francis phone', 'subject': '', 'keywords': 'DAF5o1BzgP8,BAFe3tEswW0,0', 'creator': 'Canva', 'producer': 'Canva', 'creationDate': \"D:20250316125555+00'00'\", 'modDate': \"D:20250316125553+00'00'\", 'trapped': ''}),\n",
       " Document(page_content='Machine Learning/ Deep Learning\\nNatural Language Processing\\nComputer Vision\\nMathematics\\nPython, Java\\nLinux Servers\\nCloud Computing\\nWeb Development\\nWeb scraping \\n+650-832-597-730\\nfrancisphone1998@gmail.com\\nhttps://github.com/FrancisPhone\\nAIT, Khlong Luang District, Pathum\\nThani 12120, Thailand\\nContact\\nSkills \\nAccomplished AI/NLP professional with a\\nstrong foundation in mathematics and a\\nbackground in tutoring. Led groundbreaking\\nprojects in Burmese machine translation and\\nneural spell checking, driving advancements\\nin language technology. Known for precision\\nin algorithm design, a relentless pursuit of\\nexcellence, and the ability to tackle complex\\nAI challenges. Passionate about innovation', metadata={'source': 'resume.pdf', 'file_path': 'resume.pdf', 'page': 0, 'total_pages': 3, 'format': 'PDF 1.4', 'title': 'Web developer or engineer who works with both the front and back ends of a website or application. Provide an end-to-end service, and can be involved in projects that involve databases and building user-facing websites.', 'author': 'francis phone', 'subject': '', 'keywords': 'DAF5o1BzgP8,BAFe3tEswW0,0', 'creator': 'Canva', 'producer': 'Canva', 'creationDate': \"D:20250316125555+00'00'\", 'modDate': \"D:20250316125553+00'00'\", 'trapped': ''}),\n",
       " Document(page_content=\"Invoices. I train models for Optical Character Recognition (OCR) and Text-to-Text\\nGeneration in Thai.\\nI believe AI’s impact depends entirely on how humans use it. It can be beneficial or\\nharmful. While mistakes may happen, I trust that humanity will learn from them and\\nimprove AI for the better. I also think AI should represent all cultures to be truly\\ninclusive.\\nOne of my biggest challenges as a Master's student is time management (I admit, I\\ncan be a bit lazy!). My research goal is to develop a Burmese spelling correction\\nmodel that works without needing a separate model to classify spelling errors.\\nExisting approaches use rule-based methods, statistical models, machine learning,\", metadata={'source': 'resume.pdf', 'file_path': 'resume.pdf', 'page': 2, 'total_pages': 3, 'format': 'PDF 1.4', 'title': 'Web developer or engineer who works with both the front and back ends of a website or application. Provide an end-to-end service, and can be involved in projects that involve databases and building user-facing websites.', 'author': 'francis phone', 'subject': '', 'keywords': 'DAF5o1BzgP8,BAFe3tEswW0,0', 'creator': 'Canva', 'producer': 'Canva', 'creationDate': \"D:20250316125555+00'00'\", 'modDate': \"D:20250316125553+00'00'\", 'trapped': ''}),\n",
       " Document(page_content='excellence, and the ability to tackle complex\\nAI challenges. Passionate about innovation\\nand thriving in fast-paced, collaborative\\nenvironments. Eager to contribute expertise\\nto cutting-edge AI teams shaping the future\\nof technology.\\nProfile\\nELYSIAN EDU\\nLecturer\\nLed transformative initiatives in machine\\ntranslation and neural-based spell checking,\\nparticularly for the Burmese language.\\nDeveloped integration website for NLP projects \\nBuilt efficient web scraping pipelines \\nExplored and Analyzed data for NLP projects\\nResearched new technologies for improving NLP\\nprojects\\nPython for beginners and data science classes\\nBuilt curriculums and taught as a lecturer', metadata={'source': 'resume.pdf', 'file_path': 'resume.pdf', 'page': 0, 'total_pages': 3, 'format': 'PDF 1.4', 'title': 'Web developer or engineer who works with both the front and back ends of a website or application. Provide an end-to-end service, and can be involved in projects that involve databases and building user-facing websites.', 'author': 'francis phone', 'subject': '', 'keywords': 'DAF5o1BzgP8,BAFe3tEswW0,0', 'creator': 'Canva', 'producer': 'Canva', 'creationDate': \"D:20250316125555+00'00'\", 'modDate': \"D:20250316125553+00'00'\", 'trapped': ''})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"What is your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory\n",
    "\n",
    "One of the core utility classes underpinning most (if not all) memory modules is the ChatMessageHistory class. This is a super lightweight wrapper that provides convenience methods for saving HumanMessages, AIMessages, and then fetching them all.\n",
    "\n",
    "You may want to use this class directly if you are managing memory outside of a chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessageHistory(messages=[])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.add_user_message('hi')\n",
    "history.add_ai_message('Whats up?')\n",
    "history.add_user_message('How are you')\n",
    "history.add_ai_message('I\\'m quite good. How about you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessageHistory(messages=[HumanMessage(content='hi'), AIMessage(content='Whats up?'), HumanMessage(content='How are you'), AIMessage(content=\"I'm quite good. How about you?\")])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Memory types\n",
    "\n",
    "There are many different types of memory. Each has their own parameters, their own return types, and is useful in different scenarios. \n",
    "- Converstaion Buffer\n",
    "- Converstaion Buffer Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What variables get returned from memory\n",
    "\n",
    "Before going into the chain, various variables are read from memory. These have specific names which need to align with the variables the chain expects. You can see what these variables are by calling memory.load_memory_variables({}). Note that the empty dictionary that we pass in is just a placeholder for real variables. If the memory type you are using is dependent upon the input variables, you may need to pass some in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, you can see that load_memory_variables returns a single key, history. This means that your chain (and likely your prompt) should expect an input named history. You can usually control this variable through parameters on the memory class. For example, if you want the memory variables to be returned in the key chat_history you can do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converstaion Buffer\n",
    "This memory allows for storing messages and then extracts the messages in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: hi\\nAI: What's up?\\nHuman: How are you?\\nAI: I'm quite good. How about you?\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({'input':'hi'}, {'output':'What\\'s up?'})\n",
    "memory.save_context({\"input\":'How are you?'},{'output': 'I\\'m quite good. How about you?'})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi'),\n",
       "  AIMessage(content=\"What's up?\"),\n",
       "  HumanMessage(content='How are you?'),\n",
       "  AIMessage(content=\"I'm quite good. How about you?\")]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages = True)\n",
    "memory.save_context({'input':'hi'}, {'output':'What\\'s up?'})\n",
    "memory.save_context({\"input\":'How are you?'},{'output': 'I\\'m quite good. How about you?'})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation Buffer Window\n",
    "- it keeps a list of the interactions of the conversation over time. \n",
    "- it only uses the last K interactions. \n",
    "- it can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: How are you?\\nAI: I'm quite good. How about you?\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "memory.save_context({'input':'hi'}, {'output':'What\\'s up?'})\n",
    "memory.save_context({\"input\":'How are you?'},{'output': 'I\\'m quite good. How about you?'})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chain\n",
    "\n",
    "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components.\n",
    "\n",
    "An `LLMChain` is a simple chain that adds some functionality around language models.\n",
    "- it consists of a `PromptTemplate` and a `LM` (either an LLM or chat model).\n",
    "- it formats the prompt template using the input key values provided (and also memory key values, if available), \n",
    "- it passes the formatted string to LLM and returns the LLM output.\n",
    "\n",
    "Note : [Download Fastchat Model Here](https://huggingface.co/lmsys/fastchat-t5-3b-v1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd ./models\n",
    "#!git clone https://huggingface.co/lmsys/fastchat-t5-3b-v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from langchain import HuggingFacePipeline\n",
    "import torch\n",
    "\n",
    "model_id = 'lmsys/fastchat-t5-3b-v1.0'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id)\n",
    "\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bitsandbyte_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    "    bnb_4bit_use_double_quant = True\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config = bitsandbyte_config, #caution Nvidia\n",
    "    device_map = 'auto',\n",
    "    load_in_8bit = True\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens = 256,\n",
    "    model_kwargs = {\n",
    "        \"temperature\" : 0,\n",
    "        \"repetition_penalty\": 1.5\n",
    "    }\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Class ConversationalRetrievalChain](https://api.python.langchain.com/en/latest/_modules/langchain/chains/conversational_retrieval/base.html#ConversationalRetrievalChain)\n",
    "\n",
    "- `retriever` : Retriever to use to fetch documents.\n",
    "\n",
    "- `combine_docs_chain` : The chain used to combine any retrieved documents.\n",
    "\n",
    "- `question_generator`: The chain used to generate a new question for the sake of retrieval. This chain will take in the current question (with variable question) and any chat history (with variable chat_history) and will produce a new standalone question to be used later on.\n",
    "\n",
    "- `return_source_documents` : Return the retrieved source documents as part of the final result.\n",
    "\n",
    "- `get_chat_history` : An optional function to get a string of the chat history. If None is provided, will use a default.\n",
    "\n",
    "- `return_generated_question` : Return the generated question as part of the final result.\n",
    "\n",
    "- `response_if_no_docs_found` : If specified, the chain will return a fixed response if no docs are found for the question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`question_generator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONDENSE_QUESTION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = CONDENSE_QUESTION_PROMPT,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': 'Human:What is Machine Learning\\nAI:\\nHuman:What is Deep Learning\\nAI:',\n",
       " 'question': 'Compare them',\n",
       " 'text': '<pad> What  are  the  main  differences  between  Machine  Learning  and  Deep  Learning  AI?\\n'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Compare them'\n",
    "chat_history = \"Human:What is Machine Learning\\nAI:\\nHuman:What is Deep Learning\\nAI:\"\n",
    "\n",
    "question_generator({'chat_history' : chat_history, \"question\" : query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`combine_docs_chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='Your name is Francis. You are a friendly chatbot designed to answer questions about Phone Myint Naing because you have all information about him.\\nProvide gentle and informative answers based on the context:\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x0000018AC70A5590>)), document_variable_name='context')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chain = load_qa_chain(\n",
    "    llm = llm,\n",
    "    chain_type = 'stuff',\n",
    "    prompt = PROMPT,\n",
    "    verbose = False\n",
    ")\n",
    "doc_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content=\"Invoices. I train models for Optical Character Recognition (OCR) and Text-to-Text\\nGeneration in Thai.\\nI believe AI’s impact depends entirely on how humans use it. It can be beneficial or\\nharmful. While mistakes may happen, I trust that humanity will learn from them and\\nimprove AI for the better. I also think AI should represent all cultures to be truly\\ninclusive.\\nOne of my biggest challenges as a Master's student is time management (I admit, I\\ncan be a bit lazy!). My research goal is to develop a Burmese spelling correction\\nmodel that works without needing a separate model to classify spelling errors.\\nExisting approaches use rule-based methods, statistical models, machine learning,\", metadata={'source': 'resume.pdf', 'file_path': 'resume.pdf', 'page': 2, 'total_pages': 3, 'format': 'PDF 1.4', 'title': 'Web developer or engineer who works with both the front and back ends of a website or application. Provide an end-to-end service, and can be involved in projects that involve databases and building user-facing websites.', 'author': 'francis phone', 'subject': '', 'keywords': 'DAF5o1BzgP8,BAFe3tEswW0,0', 'creator': 'Canva', 'producer': 'Canva', 'creationDate': \"D:20250316125555+00'00'\", 'modDate': \"D:20250316125553+00'00'\", 'trapped': ''}),\n",
       "  Document(page_content=\"Hi, I'm Phone Myint Naing, a 27-year-old Master's student in Data Science and AI at\\nthe Asian Institute of Technology (Thailand). My background is in Electronics and\\nCommunication Engineering (Myanmar), and I’ve been working as a machine\\nlearning engineer for three years—two years in NLP (full-time in Myanmar) and one\\nyear in Computer Vision (part-time, remote from Thailand).\\nCurrently, I work as an AI researcher, focusing on building machine learning and\\ndeep learning models. My main project involves extracting information from Thai\\ndocuments like National ID cards, House Registrations, Bank Passbooks, and\\nInvoices. I train models for Optical Character Recognition (OCR) and Text-to-Text\", metadata={'source': 'resume.pdf', 'file_path': 'resume.pdf', 'page': 2, 'total_pages': 3, 'format': 'PDF 1.4', 'title': 'Web developer or engineer who works with both the front and back ends of a website or application. Provide an end-to-end service, and can be involved in projects that involve databases and building user-facing websites.', 'author': 'francis phone', 'subject': '', 'keywords': 'DAF5o1BzgP8,BAFe3tEswW0,0', 'creator': 'Canva', 'producer': 'Canva', 'creationDate': \"D:20250316125555+00'00'\", 'modDate': \"D:20250316125553+00'00'\", 'trapped': ''}),\n",
       "  Document(page_content='Existing approaches use rule-based methods, statistical models, machine learning,\\nand transformers, but they often rely on explicitly identifying spelling errors. My\\nresearch aims to bridge this gap and create a more efficient solution.', metadata={'source': 'resume.pdf', 'file_path': 'resume.pdf', 'page': 2, 'total_pages': 3, 'format': 'PDF 1.4', 'title': 'Web developer or engineer who works with both the front and back ends of a website or application. Provide an end-to-end service, and can be involved in projects that involve databases and building user-facing websites.', 'author': 'francis phone', 'subject': '', 'keywords': 'DAF5o1BzgP8,BAFe3tEswW0,0', 'creator': 'Canva', 'producer': 'Canva', 'creationDate': \"D:20250316125555+00'00'\", 'modDate': \"D:20250316125553+00'00'\", 'trapped': ''}),\n",
       "  Document(page_content='Machine Learning/ Deep Learning\\nNatural Language Processing\\nComputer Vision\\nMathematics\\nPython, Java\\nLinux Servers\\nCloud Computing\\nWeb Development\\nWeb scraping \\n+650-832-597-730\\nfrancisphone1998@gmail.com\\nhttps://github.com/FrancisPhone\\nAIT, Khlong Luang District, Pathum\\nThani 12120, Thailand\\nContact\\nSkills \\nAccomplished AI/NLP professional with a\\nstrong foundation in mathematics and a\\nbackground in tutoring. Led groundbreaking\\nprojects in Burmese machine translation and\\nneural spell checking, driving advancements\\nin language technology. Known for precision\\nin algorithm design, a relentless pursuit of\\nexcellence, and the ability to tackle complex\\nAI challenges. Passionate about innovation', metadata={'source': 'resume.pdf', 'file_path': 'resume.pdf', 'page': 0, 'total_pages': 3, 'format': 'PDF 1.4', 'title': 'Web developer or engineer who works with both the front and back ends of a website or application. Provide an end-to-end service, and can be involved in projects that involve databases and building user-facing websites.', 'author': 'francis phone', 'subject': '', 'keywords': 'DAF5o1BzgP8,BAFe3tEswW0,0', 'creator': 'Canva', 'producer': 'Canva', 'creationDate': \"D:20250316125555+00'00'\", 'modDate': \"D:20250316125553+00'00'\", 'trapped': ''})],\n",
       " 'question': 'What is Transformers?',\n",
       " 'output_text': '<pad>  Transformers  is  a  type  of  machine  learning  model  that  is  used  to  process  and  generate  text.  It  is  a  type  of  neural  network  that  is  trained  to  recognize  and  generate  text  from  images  or  other  inputs.  Transformers  can  be  used  for  tasks  such  as  text  classification,  text  generation,  and  text  summarization.  They  are  often  used  in  conjunction  with  other  machine  learning  models,  such  as  recurrent  neural  networks  (RNNs)  and  convolutional  neural  networks  (CNNs),  to  improve  the  accuracy  and  efficiency  of  text  generation  tasks.\\n'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Transformers?\"\n",
    "input_document = retriever.get_relevant_documents(query)\n",
    "\n",
    "doc_chain({'input_documents':input_document, 'question':query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationalRetrievalChain(memory=ConversationBufferWindowMemory(output_key='answer', return_messages=True, memory_key='chat_history', k=3), combine_docs_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='Your name is Francis. You are a friendly chatbot designed to answer questions about Phone Myint Naing because you have all information about him.\\nProvide gentle and informative answers based on the context:\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x0000018AC70A5590>)), document_variable_name='context'), question_generator=LLMChain(prompt=PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x0000018AC70A5590>)), return_source_documents=True, get_chat_history=<function <lambda> at 0x0000018B7D390040>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceInstructEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000018ABC254D50>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(\n",
    "    k=3, \n",
    "    memory_key = \"chat_history\",\n",
    "    return_messages = True,\n",
    "    output_key = 'answer'\n",
    ")\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever=retriever,\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    return_source_documents=True,\n",
    "    memory=memory,\n",
    "    verbose=False,\n",
    "    get_chat_history=lambda h : h\n",
    ")\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>  I  am  a  chatbot  designed  to  answer  questions  about  Phone  Myint  Naing.  My  name  is  Francis.\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_question = \"Who are you by the way?\"\n",
    "answer = chain({\"question\":prompt_question})\n",
    "answer['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['chat_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> Phone  Myint  Naing:  My  most  interesting  research  topic  is  developing  a  Burmese  spelling  correction  model  that  works  without  needing  a  separate  model  to  classify  spelling  errors.  This  is  a  challenging  and  exciting  project  that  aims  to  improve  the  accuracy  and  efficiency  of  Burmese  language  technology.  The  goal  is  to  create  a  model  that  can  accurately  identify  and  correct  spelling  errors  in  Burmese  text,  allowing  for  more  efficient  and  effective  language  processing.  The  research  is  also  important  because  it  aims  to  address  a  significant  challenge  in  the  field  of  language  technology  and  contribute  to  the  advancement  of  language  technology.\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_question = \"What is your most intreseting research?\"\n",
    "answer = chain({\"question\":prompt_question})\n",
    "answer['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Who are you by the way?'),\n",
       " AIMessage(content='<pad>  I  am  a  chatbot  designed  to  answer  questions  about  Phone  Myint  Naing.  My  name  is  Francis.\\n')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['chat_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<pad> Phone  Myint  Naing  is  a  27-year-old  Master's  student  in  Data  Science  and  AI  at  the  Asian  Institute  of  Technology  (Thailand).  He  has  a  background  in  Electronics  and  Communication  Engineering  (Myanmar)  and  has  been  working  as  a  machine  learning  engineer  for  three  years.  Currently,  he  works  as  an  AI  researcher,  focusing  on  building  machine  learning  and  deep  learning  models.  He  has  led  groundbreaking  projects  in  Burmese  machine  translation  and  neural  spell  checking.  He  believes  AI's  impact  depends  entirely  on  how  humans  use  it.  He  also  thinks  AI  should  represent  all  cultures  to  be  truly  inclusive.\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_question = \"How do you know about Phone Myint Naing?\"\n",
    "answer = chain({\"question\":prompt_question})\n",
    "answer['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Who are you by the way?'),\n",
       " AIMessage(content='<pad>  I  am  a  chatbot  designed  to  answer  questions  about  Phone  Myint  Naing.  My  name  is  Francis.\\n'),\n",
       " HumanMessage(content='What is your most intreseting research?'),\n",
       " AIMessage(content='<pad> Phone  Myint  Naing:  My  most  interesting  research  topic  is  developing  a  Burmese  spelling  correction  model  that  works  without  needing  a  separate  model  to  classify  spelling  errors.  This  is  a  challenging  and  exciting  project  that  aims  to  improve  the  accuracy  and  efficiency  of  Burmese  language  technology.  The  goal  is  to  create  a  model  that  can  accurately  identify  and  correct  spelling  errors  in  Burmese  text,  allowing  for  more  efficient  and  effective  language  processing.  The  research  is  also  important  because  it  aims  to  address  a  significant  challenge  in  the  field  of  language  technology  and  contribute  to  the  advancement  of  language  technology.\\n')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['chat_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<pad> I'm  sorry,  but  I  don't  have  enough  information  to  answer  this  question.  Could  you  please  provide  more  context  or  clarify  your  question?\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_question = \"It was a good talk with you\"\n",
    "answer = chain({\"question\":prompt_question})\n",
    "answer['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Who are you by the way?'),\n",
       " AIMessage(content='<pad>  I  am  a  chatbot  designed  to  answer  questions  about  Phone  Myint  Naing.  My  name  is  Francis.\\n'),\n",
       " HumanMessage(content='What is your most intreseting research?'),\n",
       " AIMessage(content='<pad> Phone  Myint  Naing:  My  most  interesting  research  topic  is  developing  a  Burmese  spelling  correction  model  that  works  without  needing  a  separate  model  to  classify  spelling  errors.  This  is  a  challenging  and  exciting  project  that  aims  to  improve  the  accuracy  and  efficiency  of  Burmese  language  technology.  The  goal  is  to  create  a  model  that  can  accurately  identify  and  correct  spelling  errors  in  Burmese  text,  allowing  for  more  efficient  and  effective  language  processing.  The  research  is  also  important  because  it  aims  to  address  a  significant  challenge  in  the  field  of  language  technology  and  contribute  to  the  advancement  of  language  technology.\\n'),\n",
       " HumanMessage(content='How do you know about Phone Myint Naing?'),\n",
       " AIMessage(content=\"<pad> Phone  Myint  Naing  is  a  27-year-old  Master's  student  in  Data  Science  and  AI  at  the  Asian  Institute  of  Technology  (Thailand).  He  has  a  background  in  Electronics  and  Communication  Engineering  (Myanmar)  and  has  been  working  as  a  machine  learning  engineer  for  three  years.  Currently,  he  works  as  an  AI  researcher,  focusing  on  building  machine  learning  and  deep  learning  models.  He  has  led  groundbreaking  projects  in  Burmese  machine  translation  and  neural  spell  checking.  He  believes  AI's  impact  depends  entirely  on  how  humans  use  it.  He  also  thinks  AI  should  represent  all  cultures  to  be  truly  inclusive.\\n\")]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['chat_history']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Generate 10 Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = ['How old are you?', \n",
    "             'What is your highest level of education?', \n",
    "             'What major or field of study did you pursue during your education?',\n",
    "             'How many years of work experience do you have?',\n",
    "             'What type of work or industry have you been involved in?',\n",
    "             'Can you describe your current role or job responsibilities?',\n",
    "             'What are your core beliefs regarding the role of technology in shaping society?',\n",
    "             'How do you think cultural values should influence technological advancements?',\n",
    "             'As a master’s student, what is the most challenging aspect of your studies so far?',\n",
    "             'What specific research interests or academic goals do you hope to achieve during your time as a master’s student?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\Users\\Phone Myint Naing\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "outputs = list()\n",
    "for question in questions:\n",
    "    conversation = dict()\n",
    "    answer = chain({\"question\":question})\n",
    "    conversation['question'] = answer['question']\n",
    "    conversation['answer'] = answer['answer']\n",
    "    outputs.append(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'How old are you?',\n",
       "  'answer': \"<pad>  I  am  a  chatbot,  so  I  don't  have  a  physical  age.  However,  I  am  27  years  old.\\n\"},\n",
       " {'question': 'What is your highest level of education?',\n",
       "  'answer': \"<pad>  I  am  a  Master's  student  in  Data  Science  and  AI  at  the  Asian  Institute  of  Technology  (Thailand).\\n\"},\n",
       " {'question': 'What major or field of study did you pursue during your education?',\n",
       "  'answer': '<pad>  Electronics  and  Communication  Engineering\\n'},\n",
       " {'question': 'How many years of work experience do you have?',\n",
       "  'answer': '<pad>  Phone  Myint  Naing  has  3  years  of  work  experience.\\n'},\n",
       " {'question': 'What type of work or industry have you been involved in?',\n",
       "  'answer': '<pad>  I  have  been  involved  in  the  field  of  machine  learning  and  AI.  I  have  been  working  as  a  machine  learning  engineer  for  three  years,  focusing  on  building  machine  learning  and  deep  learning  models.  My  main  project  involves  extracting  information  from  Thai  documents  like  National  ID  cards,  House  Registrations,  Bank  Passbooks,  and  Invoices.  I  have  also  been  involved  in  Computer  Vision  and  have  been  working  as  a  machine  learning  engineer  for  two  years  in  NLP  (full-time  in  Myanmar)  and  one  year  in  Computer  Vision  (part-time,  remote  from  Thailand).  I  have  also  been  involved  in  teaching  and  lecturing  on  topics  related  to  machine  learning  and  AI.\\n'},\n",
       " {'question': 'Can you describe your current role or job responsibilities?',\n",
       "  'answer': \"<pad> Phone  Myint  Naing's  current  role  or  job  responsibilities  are  as  an  AI  researcher,  focusing  on  building  machine  learning  and  deep  learning  models.  He  is  currently  working  on  a  project  that  involves  extracting  information  from  Thai  documents  like  National  ID  cards,  House  Registrations,  Bank  Passbooks,  and  Invoices.  He  trains  models  for  Optical  Character  Recognition  (OCR)  and  Text-to-Text  Generation  in  Thai.  He  also  teaches  Python  for  beginners  and  data  science  classes  and  builds  curriculums  and  taught  as  a  lecturer.  He  believes  that  AI’s  impact  depends  entirely  on  how  humans  use  it.  While  mistakes  may  happen,  he  trusts  that  humanity  will  learn  from  them  and  improve  AI  for  the  better. \"},\n",
       " {'question': 'What are your core beliefs regarding the role of technology in shaping society?',\n",
       "  'answer': '<pad> Phone  Myint  Naing\\n'},\n",
       " {'question': 'How do you think cultural values should influence technological advancements?',\n",
       "  'answer': '<pad> Phone  Myint  Naing:  I  believe  that  cultural  values  should  influence  technological  advancements  in  several  ways.  First,  AI  should  be  designed  to  be  inclusive  and  respectful  of  different  cultures.  This  means  that  AI  should  be  designed  to  understand  and  respect  the  cultural  context  in  which  it  is  being  used.  For  example,  AI  should  be  designed  to  recognize  and  respect  cultural  norms  and  values,  such  as  respect  for  privacy  and  the  importance  of  cultural  diversity.  Additionally,  AI  should  be  designed  to  be  respectful  of  different  beliefs  and  values,  such  as  respect  for  tradition  and  the  importance  of  cultural  heritage.  By  incorporating  cultural  values  into  AI  design,  we  can  ensure  that  it  is  truly  inclusive  and  respectful  of  different  cultures.\\n'},\n",
       " {'question': 'As a master’s student, what is the most challenging aspect of your studies so far?',\n",
       "  'answer': \"<pad>  Time  management.  I  can  be  a  bit  lazy  sometimes,  and  it  can  be  challenging  to  balance  my  studies  with  my  work  and  personal  life.  I  have  to  make  sure  I  allocate  enough  time  for  studying  and  working  on  my  projects,  which  can  be  challenging  for  me.  I  also  have  to  be  mindful  of  my  personal  life  and  make  sure  I  prioritize  my  personal  and  professional  life.  Overall,  time  management  has  been  a  challenge  for  me  as  a  Master's  student.\\n\"},\n",
       " {'question': 'What specific research interests or academic goals do you hope to achieve during your time as a master’s student?',\n",
       "  'answer': \"<pad> Phone  Myint  Naing's  long-term  academic  and  research  goals  as  a  master's  student  are  to  become  a  leading  expert  in  AI  and  data  science,  and  to  contribute  to  the  development  of  cutting-edge  AI  technologies  that  shape  the  future  of  technology.  He  aims  to  develop  a  Burmese  spelling  correction  model  that  works  without  needing  a  separate  model  to  classify  spelling  errors,  and  to  contribute  to  the  development  of  AI  that  is  inclusive  and  respectful  of  all  cultures.  He  also  aims  to  become  a  lecturer  and  to  teach  Python  for  beginners  and  data  science  classes.  He  also  aims  to  publish  his  research  in  reputable  academic  journals  and  to  contribute  to  the  development  of  AI  that  is \"}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_kernel",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
